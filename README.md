When I first heard that CLIP could “understand” both images and text, I was amazed. A model that looks at a photo of a cat and instantly links it to the word cat? That sounded almost magical — until I tried it myself.

Very quickly, I realized something fascinating: CLIP isn’t perfect. It can be convinced, confused, or even tricked. The more I experimented, the more it felt like playing a strange game — me on one side, the model on the other — both trying to agree on what a picture means.

This project became less about getting the “right” answer and more about exploring the modality gap — the invisible space between how a machine reads an image and how it interprets words. Through trial and error, I learned how phrasing, structure, and even a single adjective can make CLIP “believe” an image matches a caption better than before.

In this post, I’ll walk through my experiments, show what worked (and what didn’t), and share how I tried to trick CLIP into higher scores — one caption at a time.
